# Modules -----------------------------------------------------------------------------------------------------------------#
import os
import torch

import numpy  as np
import pandas as pd

# External functions and utilities ----------------------------------------------------------------------------------------#
from loguru._logger   import Logger
from pathlib          import Path
from typing           import Optional, Dict, List, Tuple, Any
from torch.utils.data import Dataset, DataLoader

# Custom collate function for metadata -----------------------------------------------------------------------------------#
def collate_with_metadata(batch: List[Tuple]) -> Tuple[torch.Tensor, torch.Tensor, List[Dict]]:
    """
    ________________________________________________________________________________________________________________
    Custom collate function to handle batches with metadata.
    
    Use this with DataLoader when you need to access metadata along with features/targets.
    ________________________________________________________________________________________________________________
    Parameters:
    -> batch : List of tuples from dataset __getitem__
    ________________________________________________________________________________________________________________
    Returns:
    -> Tuple containing:
        - features : Stacked tensor of shape (batch_size, num_features)
        - targets  : Stacked tensor of shape (batch_size,)
        - metadata : List of metadata dicts (if present in batch)
    ________________________________________________________________________________________________________________
    Example:
        loader = DataLoader(dataset, collate_fn=collate_with_metadata)
        for batch_X, batch_y, batch_meta in loader:
            # batch_meta is a list of dicts with metadata for each sample
    ________________________________________________________________________________________________________________
    """
    if len(batch[0]) == 3:  # Has metadata
        features, targets, metadata = zip(*batch)
        return torch.stack(features), torch.stack(targets), list(metadata)
    else:  # No metadata
        features, targets = zip(*batch)
        return torch.stack(features), torch.stack(targets)

# Custom Collision system dataset -----------------------------------------------------------------------------------------#
class LEMONscDataset(Dataset):
    """
    ________________________________________________________________________________________________________________________
    PyTorch Dataset for LEMONsc project that loads tabular data from CSV files generated by dataset_pipe.py.
    ________________________________________________________________________________________________________________________
    This dataset can handle train, validation, and test partitions with the following features:
    - Automatic feature/target separation
    - Optional metadata loading (simulation paths, tags)
    - Efficient zero-copy conversion from NumPy to PyTorch tensors
    - CPU-only storage for optimal pin_memory performance
    ________________________________________________________________________________________________________________________
    """
    
    def __init__(self, csv_path: str, target_column: str = "M_MMO/M_tot", metadata_columns: Optional[List[str]] = None,
                 feature_columns : Optional[List[str]] = None,
                 exclude_columns : Optional[List[str]] = None,
                 logger          : Optional[Logger]    = None
                 ):
        """
        ____________________________________________________________________________________________________________________
        Initialize the LEMONsc Dataset.
        ____________________________________________________________________________________________________________________
        Parameters:
        -> csv_path              (str)                    : Path to the CSV file
        -> target_column         (str)                    : Name of the target column
        -> metadata_columns      (Optional[List[str]])    : Columns to store as metadata (e.g., ['or_sim_path', 'tag'])
        -> feature_columns       (Optional[List[str]])    : Specific feature columns to use. If None, uses all except 
                                                            target and metadata
        -> exclude_columns       (Optional[List[str]])    : Columns to exclude from features (in addition to target and 
                                                            metadata)
        -> logger                (Optional[Logger])       : Logger instance for info/warnings
        ____________________________________________________________________________________________________________________
        Notes:
            - Tensors are stored in CPU memory for optimal pin_memory performance with DataLoader
            - Uses torch.from_numpy() for zero-copy conversion (more efficient than torch.tensor())
        ____________________________________________________________________________________________________________________
        """
        self.csv_path         = csv_path
        self.target_column    = target_column
        self.metadata_columns = metadata_columns or []
        self.exclude_columns  = exclude_columns  or []
        self.logger           = logger
        
        # Load data
        if self.logger:
            self.logger.info(f"Initializing LEMONscDataset")
            self.logger.info(f"Loading dataset from: {csv_path}")
        
        self.df = pd.read_csv(csv_path)
        
        # Identify feature columns
        if feature_columns is not None:
            self.feature_columns = feature_columns
        else:
            # Automatically determine feature columns
            all_columns          = set(self.df.columns)
            excluded             = set([target_column] + self.metadata_columns + self.exclude_columns)
            self.feature_columns = sorted(list(all_columns - excluded))
        
        # Extract features and targets - Convert to numpy first
        features_np = self.df[self.feature_columns].values.astype(np.float32)
        targets_np  = self.df[target_column].values.astype(np.float32)
        
        # Convert to PyTorch tensors using from_numpy (zero-copy, more efficient)
        # Keep in CPU for optimal pin_memory performance in DataLoader
        self.features = torch.from_numpy(features_np)
        self.targets  = torch.from_numpy(targets_np)
        
        # Extract metadata if specified (lazy - only store column names for now)
        self._metadata_df = None
        if self.metadata_columns:
            # Only extract metadata columns if they exist
            existing_metadata_cols = [col for col in self.metadata_columns if col in self.df.columns]
            if existing_metadata_cols:
                self._metadata_df = self.df[existing_metadata_cols]
        
        if self.logger:
            self.logger.info(f"Loaded {len(self)} samples with {len(self.feature_columns)} features")
    
    def __len__(self) -> int:
        """Return the number of samples in the dataset."""
        return len(self.features)
    
    def __getitem__(self, idx: int):
        """
        ____________________________________________________________________________________________________________________
        Get a single sample from the dataset.
        ____________________________________________________________________________________________________________________
        Parameters:
        -> idx (int) : Index of the sample
        ____________________________________________________________________________________________________________________
        Returns:
        -> Tuple[torch.Tensor, torch.Tensor] : (features, target) tuple
            - features : torch.Tensor of shape (num_features,) - Feature vector
            - target   : torch.Tensor scalar - Target value
        ____________________________________________________________________________________________________________________
        Notes:
            - Returns tuple format compatible with standard PyTorch training loops
            - Tensors are on CPU (moved to GPU by DataLoader with pin_memory)
            - Direct tensor indexing (no new tensor creation - very efficient)
            - For metadata access, use get_metadata(idx) method separately
        ____________________________________________________________________________________________________________________
        """
        # Direct tensor indexing - no new object creation (most efficient)
        return self.features[idx], self.targets[idx]
    
    def get_metadata(self, idx: int) -> Optional[Dict[str, any]]:
        """
        ____________________________________________________________________________________________________________________
        Get metadata for a specific sample (if available).
        ____________________________________________________________________________________________________________________
        Parameters:
        -> idx (int) : Index of the sample
        ____________________________________________________________________________________________________________________
        Returns:
        -> Dict or None : Dictionary with metadata values, or None if no metadata available
        ____________________________________________________________________________________________________________________
        """
        if self._metadata_df is not None:
            return self._metadata_df.iloc[idx].to_dict()
        return None
    
    def get_feature_names(self) -> List[str]:
        """Return the list of feature column names."""
        return self.feature_columns
    
    def get_target_name(self) -> str:
        """Return the name of the target column."""
        return self.target_column


# Data Manager for Multi-Partition Datasets ------------------------------------------------------------------------------#
class LEMONscDataManager:
    """
    ________________________________________________________________________________________________________________________
    Manager class to handle train/val/test datasets and dataloaders for LEMONsc project.
    
    This class simplifies the workflow of:
    - Loading train, validation, and test partitions
    - Creating DataLoaders with appropriate configurations
    - Managing multiple folds for cross-validation
    ________________________________________________________________________________________________________________________
    """
    
    def __init__(self, dataset_root: str, fold: int = 0, target_column: str = "M_MMO/M_tot",
                 metadata_columns : Optional[List[str]] = None,
                 batch_size       : int                 = 512, 
                 num_workers      : int                 = 4,
                 device           : str                 = "cpu",
                 logger           : Optional[Logger]    = None 
                 ):
        """
        ____________________________________________________________________________________________________________________
        Initialize the LEMONsc Data Manager.
        ____________________________________________________________________________________________________________________
        Parameters:
        -> dataset_root   (str)              : Root directory containing fold subdirectories and test.csv
        -> fold           (int)              : Which fold to use (default: 0)
        -> target_column  (str)              : Name of the target column
        -> batch_size     (int)              : Batch size for DataLoaders
        -> num_workers    (int)              : Number of workers for data loading
        -> device         (str)              : Target device for training ('cpu' or 'cuda'). Note: tensors are stored 
                                              in CPU in dataset, device is only used for pin_memory configuration
        -> logger         (Optional[Logger]) : Logger instance
        ____________________________________________________________________________________________________________________
        """
        self.dataset_root     = Path(dataset_root)
        self.fold             = fold
        self.metadata_columns = metadata_columns 
        self.target_column    = target_column
        self.batch_size       = batch_size
        self.num_workers      = num_workers
        self.device           = device
        self.logger           = logger
        
        # Dataset paths
        self.train_path = self.dataset_root / f"{fold}_fold" / "train.csv"
        self.val_path   = self.dataset_root / f"{fold}_fold" / "val.csv"
        self.test_path  = self.dataset_root / "test.csv"
        
        # Datasets and dataloaders (initialized lazily)
        self.train_dataset : Optional[LEMONscDataset] = None
        self.val_dataset   : Optional[LEMONscDataset] = None
        self.test_dataset  : Optional[LEMONscDataset] = None
        
        self.train_loader  : Optional[DataLoader] = None
        self.val_loader    : Optional[DataLoader] = None
        self.test_loader   : Optional[DataLoader] = None
        
        if self.logger:
            self.logger.info(f"DataManager initialized for fold {fold}")
            self.logger.info(f"  Train: {self.train_path}")
            self.logger.info(f"  Val:   {self.val_path}")
            self.logger.info(f"  Test:  {self.test_path}")
    
    def setup(self, load_train: bool = True, load_val: bool = True, load_test: bool = True):
        """
        ____________________________________________________________________________________________________________________
        Load datasets and create dataloaders.
        ____________________________________________________________________________________________________________________
        Parameters:
        -> load_train (bool) : Whether to load training dataset
        -> load_val   (bool) : Whether to load validation dataset
        -> load_test  (bool) : Whether to load test dataset
        ____________________________________________________________________________________________________________________
        """
        
        # Load training dataset
        if load_train and self.train_path.exists():
            
            self.train_dataset = LEMONscDataset(
                csv_path         = str(self.train_path),
                metadata_columns = self.metadata_columns,
                target_column    = self.target_column,
                logger           = self.logger
            )
            
            # Create training dataloader with optimized settings
            self.train_loader = DataLoader(
                self.train_dataset, 
                batch_size         = self.batch_size, 
                shuffle            = True, 
                num_workers        = self.num_workers,
                pin_memory         = (self.device == 'cuda'),
                prefetch_factor    = 2 if self.num_workers > 0 else None,
                persistent_workers = True if self.num_workers > 0 else False
            )
            
            if self.logger:
                self.logger.success(f"Training dataset loaded: {len(self.train_dataset)} samples")
        
        # Load validation dataset
        if load_val and self.val_path.exists():
            self.val_dataset = LEMONscDataset(
                csv_path         = str(self.val_path), 
                target_column    = self.target_column, 
                metadata_columns = self.metadata_columns,
                logger           = self.logger
            )
            
            self.val_loader = DataLoader(
                self.val_dataset, 
                batch_size         = self.batch_size, 
                shuffle            = False, 
                num_workers        = self.num_workers,
                pin_memory         = (self.device == 'cuda'),
                prefetch_factor    = 2 if self.num_workers > 0 else None,
                persistent_workers = True if self.num_workers > 0 else False
            )      
            
            if self.logger:
                self.logger.success(f"Validation dataset loaded: {len(self.val_dataset)} samples")
        
        # Load test dataset
        if load_test and self.test_path.exists():
            self.test_dataset = LEMONscDataset(
                csv_path         = str(self.test_path), 
                target_column    = self.target_column, 
                metadata_columns = self.metadata_columns,
                logger           = self.logger
            )
            
            self.test_loader = DataLoader(
                self.test_dataset, 
                batch_size         = self.batch_size, 
                shuffle            = False, 
                num_workers        = self.num_workers,
                pin_memory         = (self.device == 'cuda'),
                prefetch_factor    = 2 if self.num_workers > 0 else None,
                persistent_workers = True if self.num_workers > 0 else False
            )
            
            if self.logger:
                self.logger.success(f"Test dataset loaded: {len(self.test_dataset)} samples")
    
    def get_feature_dim(self) -> int:
        """Get the number of features in the dataset."""
        if self.train_dataset is not None:
            return len(self.train_dataset.get_feature_names())
        elif self.val_dataset is not None:
            return len(self.val_dataset.get_feature_names())
        elif self.test_dataset is not None:
            return len(self.test_dataset.get_feature_names())
        else:
            raise ValueError("No dataset loaded. Call setup() first.")
    
    def get_feature_names(self) -> List[str]:
        """Get the names of features in the dataset."""
        if self.train_dataset is not None:
            return self.train_dataset.get_feature_names()
        elif self.val_dataset is not None:
            return self.val_dataset.get_feature_names()
        elif self.test_dataset is not None:
            return self.test_dataset.get_feature_names()
        else:
            raise ValueError("No dataset loaded. Call setup() first.")


# Example Usage -----------------------------------------------------------------------------------------------------------#
if __name__ == "__main__":
    """
    Example usage of LEMONscDataset and LEMONscDataManager.
    
    # ==========================================================================================================
    # BASIC USAGE: Training with DataManager
    # ==========================================================================================================
    from loguru import logger
    
    data_manager = LEMONscDataManager(
        dataset_root="./datasets/moccaset/moccasurvey/",
        fold=0,
        batch_size=512,
        num_workers=4,
        device='cuda' if torch.cuda.is_available() else 'cpu',
        logger=logger
    )
    
    # Load all partitions
    data_manager.setup(load_train=True, load_val=True, load_test=True)
    
    # Training loop
    for batch_X, batch_y in data_manager.train_loader:
        # batch_X: (batch_size, num_features)
        # batch_y: (batch_size,) - targets
        # Tensors arrive on GPU if device='cuda' and pin_memory=True (automatic)
        pass
    
    # Validation loop
    for batch_X, batch_y in data_manager.val_loader:
        pass
    
    # Get feature information
    num_features = data_manager.get_feature_dim()
    feature_names = data_manager.get_feature_names()
    
    print(f"Number of features: {num_features}")
    print(f"Features: {feature_names}")
    
    # ==========================================================================================================
    # ADVANCED: Manual dataset creation with metadata
    # ==========================================================================================================
    
    # For test set with metadata tracking
    test_dataset = LEMONscDataset(
        csv_path="./datasets/moccaset/moccasurvey/test.csv",
        target_column="M_MMO/M_tot",
        metadata_columns=['or_sim_path', 'tag'],
        logger=logger
    )
    
    # Access individual samples with metadata
    features, target = test_dataset[0]  # Standard access
    metadata = test_dataset.get_metadata(0)  # Separate metadata access
    print(f"Sample metadata: {metadata}")
    
    # ==========================================================================================================
    # PERFORMANCE TIPS
    # ==========================================================================================================
    # 1. Use num_workers > 0 for faster data loading (4-8 is typical)
    # 2. Set device='cuda' to enable pin_memory for faster GPU transfer
    # 3. Batch size 256-1024 works well for tabular data
    # 4. DataLoader automatically uses prefetch_factor=2 and persistent_workers for efficiency
    # 5. Apply normalization externally if needed (e.g., sklearn.preprocessing.StandardScaler)
    """
    pass


#--------------------------------------------------------------------------------------------------------------------------#
    