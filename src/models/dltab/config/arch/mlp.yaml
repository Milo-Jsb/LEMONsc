hidden_layers : [256, 128, 64, 32] 
out_features : 1
activation : 'relu'
dropout    : 0.2 
normalization : 'batch'
bias       : True